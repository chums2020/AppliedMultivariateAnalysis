The file, Set_8.csv, contains the data from a follow-up to the job search study. The file contains GRE scores (Verbal + Quantitative) upon entering graduate school, number of publications while in graduate school, length of time to complete the Ph.D. (in years), and the outcome of the job search (1=no interviews, 2=got a job, 3=interviews but no job). The variable, sample, divides the sample into two random halves. Analyze the data from sample=1 using discriminant analysis to determine how best to predict job search outcome. Use sample=2 for cross-validation. Answer the following questions.
1. How many discriminant functions are significant?
2. Comment on their relative “importance.”
3. How would you interpret the(se) function(s)?
4. How well are the original cases classified?
(a) Calculate a significance test that compares the classification to what would be expected by chance.
(b) Calculate Klecka’s tau.
5. (a) What is the most common type of misclassification?
(b) Speculate about what might account for this misclassification?
(c) What additional predictor(s) might this suggest for future analysis?
6. How well are the cases classified using the jackknife (leave-one-out) procedure?
7. How well are cases in the cross-validation sample classified?
8. Based on the analysis, what advice would you give to a student thinking about a career in academia?

\documentclass{article}
\setlength\parindent{0pt}
\usepackage{fullpage}
\setlength\parindent{24pt}
\begin{document}
\title{Homework 10\\ Psych 516}
\author{Ying-Ting Lin}
\date{11/9/15}
\maketitle
<<>>=
Data <- read.table('C:/Users/Claire/Documents/Applied Multivariate Analysis/Homework10/Set_8.csv', sep=',',header=TRUE)
library(psych)
library(cluster)
library(sciplot)
library(ggplot2)
library(DiscriMiner)
library(candisc)
library(sciplot)
library(MASS)
library(MVN)
library(ade4)
library(biotools)
library(klaR)
library(boot)
@

\section{How many discriminant functions are significant?}
We split the data into two samples, and keep our focus on the first sample for now. The maximum number of discriminant functions is the smaller of number of variables or number of groups. In this case, the number of variables is three, and the number of group (outcome) is also three. We can have  at most two discriminant functions. Both discriminant functions are significant.

\section{Comment on their relative “importance.”}
The first discriminant function is far more important. The table of canonical discriminant analysis shows us the eigenvalue for each discriminant function. The proportion of discrimination that is explained by each function is its eigenvalue divided by the sum of both functions' eigenvalues. Equivalently, we can directly check the "Percent" column in the table. The first function accounts for about 98 percent of the discrimination, while the second only 2 percent.

\section{3. How would you interpret the(se) function(s)?}
From the standardized coefficient table, the first function has a positively weighted by publication years and negatively weighted by completion years and GRE scores. 
Publication numbers and length of Ph.D. study weights positively in similiar magnitude on the second function.\\ 
\indent
From the structure matrix, we find that GRE's correlations with both functions are very low in magnitude. The first function is high when number of publications is high and when length of Ph.D. study is low.The second function is positively correlated with both publication numbers and length of Ph.D. study.\\ 
\indent
Hence, the first function is high if the observation has high publication numbers and short Ph.D. completion time. From the box plot, we infer that the first function may separate outcome 2 and 3 from outcome 1. The second function is high when both publication number and completion length are high. Again, from the box plot, we guess that the second function may separate outcome 3 from outcome 2. 

<<>>=
par(mfrow = c(1,1))
boxplot(Data1$gre~Data1$outcome, horizontal = TRUE, main = list("GRE"),xlab = list("Value"), ylab = list("Group") )
boxplot(Data1$pubs ~ Data1$outcome, horizontal = TRUE, main = list("Publication Numbers"),xlab = list("Value"), ylab = list("Group") )
boxplot(Data1$years ~ Data1$outcome, horizontal = TRUE, main = list("Years"),xlab = list("Value"), ylab = list("Group") )

Data1 <- Data[Data$sample ==1,]
LDA1 <- lda(outcome ~ gre + pubs + years, data = Data1)
MLM1 <- lm(cbind(gre, pubs, years)~ as.factor(outcome), data = Data1)
CDA1 <- candisc(MLM1, data = Data1)

CDA1$coeffs.std  #weight, based on standardized variables
CDA1$structure   #correlation b/w each variable and each function

CDA1
@

\section{4. How well are the original cases classified?}
The error rate (i.e., percentage of misclassification) is 0.16.
\subsection{Calculate a significance test that compares the classification to what would be
expected by chance.}
We calculate the t test. The t statistics is 15.09. 
\subsection{Calculate Klecka's tau.}
Klecka's tau equals 0.74.

<<>>=

#homogeneity and multivariate normality fails --> bootstrapping method
boxM(Data1[, 1:3], Data1$outcome)
shapiro.test(Data1$gre)
shapiro.test(Data1$pubs)
shapiro.test(Data1$years)
mardiaTest(Data[,1:3], cov = TRUE, qqplot = FALSE)
hzTest(Data[,1:3], cov = TRUE, qqplot = FALSE)

@

\end{document}
